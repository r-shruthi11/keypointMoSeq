{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "deluxe-waste",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/calebweinreb/keypointMoSeq/blob/user_friendly_pipeline/examples/keypointMoSeq_demo_colab.ipynb)\n",
    "## Keypoint MoSeq \n",
    "\n",
    "#### This notebook illustrates how to fit a keypoint-SLDS model using pose tracking data (e.g. from DeepLabCut)\n",
    "\n",
    "- Make sure keypoint_moseq is [installed](XXX)\n",
    "- Change the kernel to `keypoint_moseq`\n",
    "- Download the [example data](https://drive.google.com/drive/folders/1UNHQ_XCQEKLPPSjGspRopWBj6-YNDV6G?usp=share_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varying-morrison",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.config import config\n",
    "config.update('jax_enable_x64', True)\n",
    "import keypoint_moseq as kpm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preliminary-agency",
   "metadata": {},
   "source": [
    "### Setup a new project\n",
    "- Edit `project_dir` and `dlc_config` below so that `dlc_config` points to the example data.\n",
    "- This step creates a new project directory with a MoSeq `config.yml` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intermediate-kenya",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = 'demo_project'\n",
    "\n",
    "# option 1: set up from scratch\n",
    "# kpm.setup_project(project_dir)\n",
    "\n",
    "# option 2: set up from deeplabcut\n",
    "dlc_config = 'moseq_example-caleb-2022-11-09/config.yaml'\n",
    "kpm.setup_project(project_dir, deeplabcut_config=dlc_config)\n",
    "\n",
    "# define config loader\n",
    "config = lambda: kpm.load_config(project_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gothic-viking",
   "metadata": {},
   "source": [
    "### Edit the config file\n",
    "\n",
    "- The config can be edited manually, or using `kpm.update_config`, as shown below.\n",
    "- The cell below contains all config edits necessary to run this notebook on the example data.\n",
    "\n",
    "In general, the following parameters should be specified for each project:\n",
    "\n",
    "- `bodyparts` (The name of each keypoint; automatically imported if you setup from DLC)\n",
    "- `use_bodyparts` (Subset of bodyparts to use for modeling; defaults to all if you setup from DLC)\n",
    "- `anterior_bodyparts` and `posterior_bodyparts` (Used for rotational alignment)\n",
    "- `video_dir` (directory with videos of each experiment; detaults to `[DLC project path]/videos` if you setup from DLC.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "theoretical-yahoo",
   "metadata": {},
   "outputs": [],
   "source": [
    "kpm.update_config(\n",
    "    project_dir,\n",
    "    use_bodyparts=['spine4','spine3','spine2','spine1','head','nose','right ear','left ear'],\n",
    "    anterior_bodyparts=['nose'], posterior_bodyparts=['spine4'],\n",
    "    latent_dimension=4, slope= -0.47, intercept= 0.236721,\n",
    "    # video_dir = 'moseq_example-caleb-2022-11-09/videos',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phantom-dating",
   "metadata": {},
   "source": [
    "### Load data\n",
    "\n",
    "Data can be loaded directly from DLC or from any another sources as long as it has the following format:\n",
    "- `coordinates`: dict of (T,K,D) arrays where K is the number of keypoints and D is 2 or 3\n",
    "- `confidences`: dict of (T,K) arrays of **nonzero** confidence scores for each keypoint\n",
    "\n",
    "If applying keypoint-MoSeq to your own data, note that:\n",
    "- `confidences` are optional (they are used to set the error prior for each observation)\n",
    "- if importing from DLC, data are assumed to be .h5/.csv files in `video_dir`\n",
    "- each key in `coordinates` should start with its video name \n",
    "    - e.g. `coordinates[\"experiment1etc\"]` would correspond to `experiment1.avi`\n",
    "    - in general this will already be true if importing from DLC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expressed-christian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from DLC\n",
    "coordinates,confidences = kpm.load_keypoints_from_deeplabcut(**config())\n",
    "\n",
    "# format for modeling (reshapes into fixed-length batches and moves to GPU)\n",
    "data,batch_info = kpm.format_data(coordinates, confidences=confidences, **config())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "processed-struggle",
   "metadata": {},
   "source": [
    "### Calibrate \n",
    "\n",
    "The purpose of calibration is to learn the relationship between error and keypoint confidence scores. The resulting regression coefficients (`slope` and `intercept`) are used during modeling to set the noise prior on a per-frame, per-keypoint basis. One can also adjust the `confidence_threshold` parameter at this step, which is used to define outlier keypoints for PCA and model initialization. **This step can be skipped for the demo data** since the config already includes suitable regression coefficients.\n",
    "\n",
    "- Run the cell below. A widget should appear with a video frame in the center.\n",
    "    - *If the widget doesn't render, try using jupyter lab instead of jupyter notebook*\n",
    "    \n",
    "\n",
    "- Annotate each frame with the correct location of the labeled bodypart\n",
    "    - Left click to specify the correct location - an \"X\" should appear.\n",
    "    - Use the arrow buttons and/or sample slider on the left to annotate additional frames.\n",
    "    - Each annotation adds a point to the right-hand scatter plot. Continue until the regression line stabilizes.\n",
    "    \n",
    "    \n",
    "- At any point, adjust the confidence threshold by clicking on the scatter plot.\n",
    "\n",
    "\n",
    "- **Use the \"save\" button to update the config and store your annotations to disk**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chicken-diameter",
   "metadata": {},
   "outputs": [],
   "source": [
    "kpm.noise_calibration(project_dir, coordinates, confidences, **config())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organizational-theorem",
   "metadata": {},
   "source": [
    "### Fit PCA\n",
    "\n",
    "Run the cell below to fit a PCA model to aligned keypoint coordinates. After fitting, two plots are generated: a cumulative [scree plot](https://en.wikipedia.org/wiki/Scree_plot) and a depiction of each PC, where translucent nodes/edges represent the mean pose and opaque nodes/edges represent a perturbation in the direction of the PC. \n",
    "\n",
    "- After fitting, edit `latent_dimension` in the config. A good heuristic is the number of dimensions needed to explain 90% of variance. \n",
    "- If your computer crashes at this step, try lowering `PCA_fitting_num_frames` in the config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respiratory-canvas",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment if you have already fit pca\n",
    "# pca = kpm.load_pca(project_dir)\n",
    "\n",
    "pca = kpm.fit_pca(**data, **config())\n",
    "kpm.save_pca(pca, project_dir)\n",
    "\n",
    "kpm.print_dims_to_explain_variance(pca, 0.9)\n",
    "kpm.plot_scree(pca, project_dir=project_dir)\n",
    "kpm.plot_pcs(pca, project_dir=project_dir, **config())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accomplished-pantyhose",
   "metadata": {},
   "source": [
    "## Fitting MoSeq\n",
    "\n",
    "Fitting a MoSeq model includes the following steps:\n",
    "1. **Initialization:** Auto-regressive (AR) parameters and syllable sequences are randomly initialized using pose trajectories from PCA.\n",
    "2. **Fitting an AR-HMM:** The AR parameters, transition probabilities and syllable sequences are iteratively updated through Gibbs sampling. \n",
    "3. **Fitting keypoint-SLDS:** All parameters, including both the AR-HMM as well as centroid, heading, noise-estimates and continuous latent states (i.e. PCA trajectories) are iteratively updated through Gibbs sampling. This step is especially useful for noisy data.\n",
    "4. **Apply the model:** The learned model parameters are used to infer a syllable sequence for each experiment. This step should always be applied at the end of model fitting, and it can also be used later on to infer syllable sequences for newly added data.\n",
    "\n",
    "### Setting hyperparameters\n",
    "\n",
    "There are two ways to change hyperparameters:\n",
    "1. Update the config using `kpm.update_config(param_name=NUMBER)` and then re-initialize the model\n",
    "2. Change the model directly via `kpm.update_hypparams(model/checkpoint, param_name=NUMBER)`\n",
    "\n",
    "In general, the main hyperparam that needs to be adjusted is **kappa**, which sets the time-scale of syllables. Higher kappa leads to longer syllables. For this tutorial we chose kappa values that yielded a median syllable duration of 400ms (12 frames). In general, you will need to tune kappa for each new dataset based on the intended syllable time-scale. **You will need to pick two kappas: one for AR-HMM fitting and one for keypoint-SLDS.**\n",
    "- We recommend iteratively updating kappa and refitting the model until the target syllable time-scale is attained.  \n",
    "- Model fitting can be stopped at any time by interrupting the kernel, and kappa can be adjusted as described above.\n",
    "- Keypoint-SLDS will generally require a lower value of kappa to yield the same target syllable durations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utility-penetration",
   "metadata": {},
   "source": [
    "### Step 1: Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "found-administrator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optionally update kappa in the config before initializing \n",
    "# model = kpm.update_config(kappa=NUMBER)\n",
    "\n",
    "# initialize the model\n",
    "model = kpm.initialize_model(pca=pca, **data, **config())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "partial-remove",
   "metadata": {},
   "source": [
    "### Step 2: Fitting an AR-HMM\n",
    "\n",
    "In addition to fitting an AR-HMM, the function below will...\n",
    "- generate a name for the model and a new directory inside `project_dir` where outputs will be saved\n",
    "- save a checkpoint every 10 iterations from which fitting can be restarted\n",
    "    - a single checkpoint file contains the full history of fitting, and can be used to restart fitting from any iteration\n",
    "- plot the progress of fitting every 10 iterations, including\n",
    "    - the distributions of syllable frequencies and durations for the most recent iteration\n",
    "    - the change in median syllable duration across fitting iterations\n",
    "    - the change in syllable sequence across fitting iterations in a random window (a new window is selected each time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888e6ff7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model,history,name = kpm.fit_model(model, data, batch_info, ar_only=True, \n",
    "                                   num_iters=50, project_dir=project_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thorough-identity",
   "metadata": {},
   "source": [
    "### Step 3: Fitting keypoint-SLDS\n",
    "\n",
    "The following code fits a keypoint-SLDS model, using the results of AR-HMM fitting for initialization\n",
    "- If using your own data, you may need to try a few values of kappa at this step. \n",
    "- Use `kpm.revert` to resume from the same starting point each time you restart fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swiss-repeat",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model checkpoint generated during step 2 (AR-HMM fitting)\n",
    "checkpoint = kpm.load_checkpoint(project_dir=project_dir, name=name)\n",
    "\n",
    "# the following will cause fitting to resume from iteration 50, rather than the most recent iteration\n",
    "# checkpoint = kpm.revert(checkpoint, 50)\n",
    "\n",
    "# update kappa to maintain the desired syllable time-scale\n",
    "checkpoint = kpm.update_hypparams(checkpoint, kappa=7e4)\n",
    "\n",
    "model,history,name = kpm.resume_fitting(**checkpoint, project_dir=project_dir, \n",
    "                                        ar_only=False, num_iters=200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empty-houston",
   "metadata": {},
   "source": [
    "### Step 4: Apply the model\n",
    "\n",
    "The code below infers a final syllable sequence for each experiment. The results are saved in `project_dir/name/results.h5`. \n",
    "- The default assumption is that `coordinates` and `confidences` are the same data that were used for model fitting.\n",
    "\n",
    "To infer syllable sequences for new data:\n",
    "- Run ``kpm.apply_model`` with `use_saved_states=False` and pass in a pca model\n",
    "- Results for the new experiments will be added to the existing `results.h5` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electric-combine",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = kpm.load_checkpoint(project_dir=project_dir, name=name)\n",
    "\n",
    "results = kpm.apply_model(coordinates=coordinates, confidences=confidences, \n",
    "                          project_dir=project_dir, **checkpoint, **config())\n",
    "\n",
    "# use this command if applying the model to new data\n",
    "# results = kpm.apply_model(coordinates=coordinates, confidences=confidences, \n",
    "#                           use_saved_states=False, pca=kpm.load_pca(project_dir),\n",
    "#                           project_dir=project_dir, **checkpoint, **config())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breeding-fashion",
   "metadata": {},
   "source": [
    "### Visualize results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subject-disney",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate plots showing the average keypoint trajectories for each syllable\n",
    "kpm.generate_trajectory_plots(coordinates, name=name, project_dir=project_dir, **config())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominant-packet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate video clips for each syllable\n",
    "kpm.generate_crowd_movies(name=name, project_dir=project_dir, **config())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8246a08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
